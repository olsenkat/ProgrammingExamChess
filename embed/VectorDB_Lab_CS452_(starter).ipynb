{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olsenkat/ProgrammingExamChess/blob/main/embed/VectorDB_Lab_CS452_(starter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4UOUNsUTsvcn"
      },
      "outputs": [],
      "source": [
        "# Download datasets from kaggle\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"lex-fridman-text-embedding-3-large-128.zip\"):\n",
        "  kaggle_json = {\"username\": \"michaeltreynolds\",\"key\": \"149701be742f30a8a0526762c61beea0\"}\n",
        "  kaggle_dir = os.path.join(os.path.expanduser(\"~\"), \".kaggle\")\n",
        "  os.makedirs(kaggle_dir, exist_ok=True)\n",
        "  kaggle_config_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "  with open(kaggle_config_path, 'w') as f:\n",
        "    json.dump(kaggle_json, f)\n",
        "\n",
        "  !kaggle datasets download -d michaeltreynolds/lex-fridman-text-embedding-3-large-128\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip kaggle data\n",
        "\n",
        "!unzip lex-fridman-text-embedding-3-large-128.zip\n",
        "!unzip lex-fridman-text-embedding-3-large-128/*.zip\n"
      ],
      "metadata": {
        "id": "h3swnD70x4FG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c646ade-932d-4d6f-902e-e1b43be35af6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  lex-fridman-text-embedding-3-large-128.zip\n",
            "replace documents/documents/batch_request_0lw3vrQqdWbdBRurTGNMHU76.jsonl? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: documents/documents/batch_request_0lw3vrQqdWbdBRurTGNMHU76.jsonl  \n",
            "  inflating: documents/documents/batch_request_3GozevpriRRzieX4za9xfNmY.jsonl  \n",
            "  inflating: documents/documents/batch_request_3maYxwEF1svWRpbYr10br6Wv.jsonl  \n",
            "  inflating: documents/documents/batch_request_7hQA9m3pUXx22JXInjYZNAu2.jsonl  \n",
            "  inflating: documents/documents/batch_request_7oR3UbWsHgESFLr5eqL3jkMD.jsonl  \n",
            "  inflating: documents/documents/batch_request_CXU6VbN4SDinplgj1MpILc1u.jsonl  \n",
            "  inflating: documents/documents/batch_request_Fve0NjohSf5Qe0bZtAnp8D5A.jsonl  \n",
            "  inflating: documents/documents/batch_request_If8QibqlgU0XRU5FcD5zpiuL.jsonl  \n",
            "  inflating: documents/documents/batch_request_KQAdZBdQHYMI2MfEMXf3ytLM.jsonl  \n",
            "  inflating: documents/documents/batch_request_MFMHpnaCSkNrbgAgOxCzaLOl.jsonl  \n",
            "  inflating: documents/documents/batch_request_NDF2wpJfxtprLcfkIUYE2iWQ.jsonl  \n",
            "  inflating: documents/documents/batch_request_S6oh8W0n2tNadcBvYxOzzYNx.jsonl  \n",
            "  inflating: documents/documents/batch_request_WsWMgjQhn3wXtMZcXuKc1ZE7.jsonl  \n",
            "  inflating: documents/documents/batch_request_Z6MhxEvYsKphmnJTno6L6QkW.jsonl  \n",
            "  inflating: documents/documents/batch_request_gWb7SDYIzTMTN4plMW2auahA.jsonl  \n",
            "  inflating: documents/documents/batch_request_hE3eSd3c5AQWcMWpaV0dBJPh.jsonl  \n",
            "  inflating: documents/documents/batch_request_n6Q1PS1f6wiNnWJd6qzIcbKf.jsonl  \n",
            "  inflating: embedding/embedding/0lw3vrQqdWbdBRurTGNMHU76.jsonl  \n",
            "  inflating: embedding/embedding/3GozevpriRRzieX4za9xfNmY.jsonl  \n",
            "  inflating: embedding/embedding/3maYxwEF1svWRpbYr10br6Wv.jsonl  \n",
            "  inflating: embedding/embedding/7hQA9m3pUXx22JXInjYZNAu2.jsonl  \n",
            "  inflating: embedding/embedding/7oR3UbWsHgESFLr5eqL3jkMD.jsonl  \n",
            "  inflating: embedding/embedding/CXU6VbN4SDinplgj1MpILc1u.jsonl  \n",
            "  inflating: embedding/embedding/Fve0NjohSf5Qe0bZtAnp8D5A.jsonl  \n",
            "  inflating: embedding/embedding/If8QibqlgU0XRU5FcD5zpiuL.jsonl  \n",
            "  inflating: embedding/embedding/KQAdZBdQHYMI2MfEMXf3ytLM.jsonl  \n",
            "  inflating: embedding/embedding/MFMHpnaCSkNrbgAgOxCzaLOl.jsonl  \n",
            "  inflating: embedding/embedding/NDF2wpJfxtprLcfkIUYE2iWQ.jsonl  \n",
            "  inflating: embedding/embedding/S6oh8W0n2tNadcBvYxOzzYNx.jsonl  \n",
            "  inflating: embedding/embedding/WsWMgjQhn3wXtMZcXuKc1ZE7.jsonl  \n",
            "  inflating: embedding/embedding/Z6MhxEvYsKphmnJTno6L6QkW.jsonl  \n",
            "  inflating: embedding/embedding/gWb7SDYIzTMTN4plMW2auahA.jsonl  \n",
            "  inflating: embedding/embedding/hE3eSd3c5AQWcMWpaV0dBJPh.jsonl  \n",
            "  inflating: embedding/embedding/n6Q1PS1f6wiNnWJd6qzIcbKf.jsonl  \n",
            "unzip:  cannot find or open lex-fridman-text-embedding-3-large-128/*.zip, lex-fridman-text-embedding-3-large-128/*.zip.zip or lex-fridman-text-embedding-3-large-128/*.zip.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use specific libraries\n",
        "!pip install datasets==2.20.0 psycopg2==2.9.9 pgcopy==1.6.0\n",
        "import psycopg2"
      ],
      "metadata": {
        "id": "SYDFzWfv4HLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb48340-f088-4833-9b74-f78c9e30d56c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.20.0\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting psycopg2==2.9.9\n",
            "  Downloading psycopg2-2.9.9.tar.gz (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pgcopy==1.6.0\n",
            "  Downloading pgcopy-1.6.0-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.20.0)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.70.16)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (3.12.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==2.20.0) (6.0.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from pgcopy==1.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.20.0) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets==2.20.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==2.20.0) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.20.0) (1.17.0)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pgcopy-1.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Building wheels for collected packages: psycopg2\n",
            "  Building wheel for psycopg2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for psycopg2: filename=psycopg2-2.9.9-cp312-cp312-linux_x86_64.whl size=533259 sha256=98f253fd2cd286f5414c59238a61c14936c57e8f43589820777a1e0250279993\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/ac/80/7ccec163e3d05ae2112311b895132409b9abfd7e0c1c6b5183\n",
            "Successfully built psycopg2\n",
            "Installing collected packages: pyarrow-hotfix, psycopg2, fsspec, pgcopy, datasets\n",
            "  Attempting uninstall: psycopg2\n",
            "    Found existing installation: psycopg2 2.9.10\n",
            "    Uninstalling psycopg2-2.9.10:\n",
            "      Successfully uninstalled psycopg2-2.9.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 fsspec-2024.5.0 pgcopy-1.6.0 psycopg2-2.9.9 pyarrow-hotfix-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your own trial account at timescaledb and paste your own connection string\n",
        "\n",
        "#TODO\n",
        "CONNECTION = \"<YOUR CONNECTION>\""
      ],
      "metadata": {
        "id": "ukT4dY-z25XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use this if you want to start over on your postgres table!\n",
        "\n",
        "DROP_TABLE = \"DROP TABLE IF EXISTS podcast, segment\"\n",
        "with psycopg2.connect(CONNECTION) as conn:\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(DROP_TABLE)\n",
        "    conn.commit() # Commit the changes\n"
      ],
      "metadata": {
        "id": "gpp_3EuU3SN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful function that takes a pd.DataFrame and copies it directly into a table.\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import psycopg2\n",
        "\n",
        "from typing import List\n",
        "\n",
        "def fast_pg_insert(df: pd.DataFrame, connection: str, table_name: str, columns: List[str]) -> None:\n",
        "    \"\"\"\n",
        "        Inserts data from a pandas DataFrame into a PostgreSQL table using the COPY command for fast insertion.\n",
        "\n",
        "        Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing the data to be inserted.\n",
        "        connection (str): The connection string to the PostgreSQL database.\n",
        "        table_name (str): The name of the target table in the PostgreSQL database.\n",
        "        columns (List[str]): A list of column names in the target table that correspond to the DataFrame columns.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    conn = psycopg2.connect(connection)\n",
        "    _buffer = io.StringIO()\n",
        "    df.to_csv(_buffer, sep=\";\", index=False, header=False)\n",
        "    _buffer.seek(0)\n",
        "    with conn.cursor() as c:\n",
        "        c.copy_from(\n",
        "            file=_buffer,\n",
        "            table=table_name,\n",
        "            sep=\";\",\n",
        "            columns=columns,\n",
        "            null=''\n",
        "        )\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "wZDxdvoP4Fov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Schema\n",
        "We will create a database with two tables: podcast and segment:\n",
        "\n",
        "**podcast**\n",
        "\n",
        "- PK: id\n",
        " - The unique podcast id found in the huggingface data (i,e., TRdL6ZzWBS0  is the ID for Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "- title\n",
        " - The title of podcast (ie., Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214)\n",
        "\n",
        "**segment**\n",
        "\n",
        "- PK: id\n",
        " - the unique identifier for the podcast segment. This was created by concatenating the podcast idx and the segment index together (ie., \"0;1\") is the 0th podcast and the 1st segment\n",
        "This is present in the as the \"custom_id\" field in the `embedding.jsonl` and batch_request.jsonl files\n",
        "- start_time\n",
        " - The start timestamp of the segment\n",
        "- end_time\n",
        " - The end timestamp of the segment\n",
        "- content\n",
        " - The raw text transcription of the podcast\n",
        "- embedding\n",
        " - the 128 dimensional vector representation of the text\n",
        "- FK: podcast_id\n",
        " - foreign key to podcast.id"
      ],
      "metadata": {
        "id": "7Y2HkhMZmHFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample document:\n",
        "# {\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"url\": \"/v1/embeddings\",\n",
        "#   \"method\": \"POST\",\n",
        "#   \"body\": {\n",
        "#     \"input\": \" have been possible without these approaches?\",\n",
        "#     \"model\": \"text-embedding-3-large\",\n",
        "#     \"dimensions\": 128,\n",
        "#     \"metadata\": {\n",
        "#       \"title\": \"Podcast: Boris Sofman: Waymo, Cozmo, Self-Driving Cars, and the Future of Robotics | Lex Fridman Podcast #241\",\n",
        "#       \"podcast_id\": \"U_AREIyd0Fc\",\n",
        "#       \"start_time\": 484.52,\n",
        "#       \"stop_time\": 487.08\n",
        "#     }\n",
        "#   }\n",
        "# }\n",
        "\n",
        "# Sample embedding:\n",
        "# {\n",
        "#   \"id\": \"batch_req_QZBmHS7FBiVABxcsGiDx2THJ\",\n",
        "#   \"custom_id\": \"89:115\",\n",
        "#   \"response\": {\n",
        "#     \"status_code\": 200,\n",
        "#     \"request_id\": \"7a55eba082c70aca9e7872d2b694f095\",\n",
        "#     \"body\": {\n",
        "#       \"object\": \"list\",\n",
        "#       \"data\": [\n",
        "#         {\n",
        "#           \"object\": \"embedding\",\n",
        "#           \"index\": 0,\n",
        "#           \"embedding\": [\n",
        "#             0.0035960325,\n",
        "#             126 more lines....\n",
        "#             -0.093248844\n",
        "#           ]\n",
        "#         }\n",
        "#       ],\n",
        "#       \"model\": \"text-embedding-3-large\",\n",
        "#       \"usage\": {\n",
        "#         \"prompt_tokens\": 7,\n",
        "#         \"total_tokens\": 7\n",
        "#       }\n",
        "#     }\n",
        "#   },\n",
        "#   \"error\": null\n",
        "# }"
      ],
      "metadata": {
        "id": "3EZuFdc9m9uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "dir = \"documents/documents\"\n",
        "docs = os.listdir(dir)\n",
        "podcast = {}\n",
        "for doc in docs:\n",
        "  with open(os.path.join(dir, doc)) as input_file:\n",
        "    for line in input_file:\n",
        "      data = json.loads(line)\n",
        "      podcast[data[\"body\"][\"metadata\"][\"podcast_id\"]] = data[\"body\"][\"metadata\"][\"title\"]\n",
        "\n",
        "      break;\n",
        "print(podcast)\n",
        "\n"
      ],
      "metadata": {
        "id": "onRLqDDzCFpo",
        "outputId": "2394ed78-c58f-4773-dcf7-90afe3596510",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tueAcSiiqYA': 'Podcast: Jordan Ellenberg: Mathematics of High-Dimensional Shapes and Geometries | Lex Fridman Podcast #190', 'iZRbD7q1n-U': 'Podcast: John Danaher: Grappling, Jiu Jitsu, ADCC, and Animal Combat | Lex Fridman Podcast #328', 'rIAZJNe7YtE': 'Podcast: Eric Weinstein: Geometric Unity and the Call for New Ideas & Institutions | Lex Fridman Podcast #88', 'VeH7qKZr0WI': 'Podcast: Balaji Srinivasan: How to Fix Government, Twitter, Science, and the FDA | Lex Fridman Podcast #331', 'G4hL5Om4IJ4': 'Podcast: Jim Keller: The Future of Computing, AI, Life, and Consciousness | Lex Fridman Podcast #162', 'RvhpncC5jZ8': 'Podcast: John Clarke: The Art of Fighting and the Pursuit of Excellence | Lex Fridman Podcast #143', 'TRdL6ZzWBS0': 'Podcast: Jed Buchwald: Isaac Newton and the Philosophy of Science | Lex Fridman Podcast #214', '7Grseeycor4': 'Podcast: Tyler Cowen: Economic Growth & the Fight Against Conformity & Mediocrity | Lex Fridman Podcast #174', 'tOTenjh_8hw': 'Podcast: Bobby Lee: Comedy, Skyrim, Sex Robots, Love, Fame, and Power | Lex Fridman Podcast #287', 'ZPPAOakITeQ': 'Podcast: Sebastian Thrun: Flying Cars, Autonomous Vehicles, and Education | Lex Fridman Podcast #59', 'nDDJFvuFXdc': 'Podcast: Peter Woit: Theories of Everything & Why String Theory is Not Even Wrong | Lex Fridman Podcast #246', 'ICj8p5jPd3Y': 'Podcast: Matthew Johnson: Psychedelics | Lex Fridman Podcast #145', 'H9AAnV59ddE': 'Podcast: Todd Howard: Skyrim, Elder Scrolls 6, Fallout, and Starfield | Lex Fridman Podcast #342', 'nhGwJLXzHs8': 'Podcast: Brian Keating: Cosmology, Astrophysics, Aliens & Losing the Nobel Prize | Lex Fridman Podcast #257', 'Ui38ZzTymDY': 'Podcast: Jay McClelland: Neural Networks and the Emergence of Cognition | Lex Fridman Podcast #222', 'TG6BuSjwP4o': 'Podcast: Bret Weinstein: Truth, Science, and Censorship in the Time of a Pandemic | Lex Fridman Podcast #194', 'U_AREIyd0Fc': 'Podcast: Boris Sofman: Waymo, Cozmo, Self-Driving Cars, and the Future of Robotics | Lex Fridman Podcast #241'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create table statements that you'll write\n",
        "#TODO\n",
        "\n",
        "\n",
        "# may need to run this to enable vector data type if you didn't select AI in service\n",
        "# CREATE_EXTENSION = \"CREATE EXTENSION vector\"\n",
        "\n",
        "# TODO: Add create table statement\n",
        "CREATE_PODCAST_TABLE = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# id and title from document - want unique set of titles and podcast id\n",
        "\n",
        "# TODO: Add create table statement\n",
        "CREATE_SEGMENT_TABLE = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "# TODO: Create tables with psycopg2 (example: https://www.geeksforgeeks.org/executing-sql-query-with-psycopg2-in-python/)\n",
        "\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n"
      ],
      "metadata": {
        "id": "bU6fFAwb5EYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract needed data out of JSONL files. This may be the hard part!\n",
        "\n",
        "# TODO: What data do we need?\n",
        "# TODO: What data is in the documents jsonl files?\n",
        "# TODO: What data is in the embedding jsonl files?\n",
        "# TODO: Get some pandas data frames for our two tables so we can copy the data in!\n",
        "\n"
      ],
      "metadata": {
        "id": "v81052OY5BKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Optional #####\n",
        "# In addition to the embedding and document files you might like to load\n",
        "# the full podcast raw data via the hugging face datasets library\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# ds = load_dataset(\"Whispering-GPT/lex-fridman-podcast\")\n"
      ],
      "metadata": {
        "id": "xo3Y8IHYRruE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"podcast\" data into the podcast postgres table!\n"
      ],
      "metadata": {
        "id": "5W3f-2iTpGL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Copy all the \"segment\" data into the segment postgres table!\n",
        "# HINT 1: use the recommender.utils.fast_pg_insert function to insert data into the database\n",
        "# otherwise inserting the 800k documents will take a very, very long time\n",
        "# HINT 2: if you don't want to use all your memory and crash\n",
        "# colab, you'll need to either send the data up in chunks\n",
        "# or write your own function for copying it up. Alternative to chunking maybe start\n",
        "# with writing it to a CSV and then copy it up?\n"
      ],
      "metadata": {
        "id": "ZTUsciGfpahF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This script is used to query the database\n",
        "import os\n",
        "import psycopg2\n",
        "\n",
        "\n",
        "# Write your queries\n",
        "# Q1) What are the five most similar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n",
        "\n",
        "conn = psycopg2.connect(CONNECTION)\n",
        "cur = conn.cursor()\n",
        "cur.execute(\"\"\"\n",
        "\n",
        "\"\"\")\n",
        "for row in cur.fetchall():\n",
        "  print(row)\n",
        "\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "NvkG-51G5IDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2) What are the five most dissimilar segments to segment \"267:476\"\n",
        "# Input: \"that if we were to meet alien life at some point\"\n",
        "# For each result return the podcast name, the segment id, segment raw text, the start time, stop time, and embedding distance\n"
      ],
      "metadata": {
        "id": "Dq8ePSfrw8Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3) What are the five most similar segments to segment '48:511'\n",
        "\n",
        "# Input: \"Is it is there something especially interesting and profound to you in terms of our current deep learning neural network, artificial neural network approaches and the whatever we do understand about the biological neural network.\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n"
      ],
      "metadata": {
        "id": "dmTK02bZk3pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4) What are the five most similar segments to segment '51:56'\n",
        "\n",
        "# Input: \"But what about like the fundamental physics of dark energy? Is there any understanding of what the heck it is?\"\n",
        "# For each result return the podcast name, the segment id, segment raw text,  the start time, stop time, and embedding distance\n"
      ],
      "metadata": {
        "id": "jcfhAKKQk9rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5) For each of the following podcast segments, find the five most similar podcast episodes. Hint: You can do this by averaging over the embedding vectors within a podcast episode.\n",
        "\n",
        "#     a) Segment \"267:476\"\n",
        "\n",
        "#     b) Segment '48:511'\n",
        "\n",
        "#     c) Segment '51:56'\n",
        "\n",
        "# For each result return the Podcast title and the embedding distance\n"
      ],
      "metadata": {
        "id": "OT4yTTn4k_iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6) For podcast episode id = VeH7qKZr0WI, find the five most similar podcast episodes. Hint: you can do a similar averaging procedure as Q5\n",
        "\n",
        "# Input Episode: \"Balaji Srinivasan: How to Fix Government, Twitter, Science, and the FDA | Lex Fridman Podcast #331\"\n",
        "# For each result return the Podcast title and the embedding distance\n"
      ],
      "metadata": {
        "id": "_oKIVjn4lBYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deliverables\n",
        "You will turn in a ZIP or PDF file containing all your code and a PDF file with the queries and results for questions 1-7."
      ],
      "metadata": {
        "id": "WBZVtZP4lDO2"
      }
    }
  ]
}